{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Information retrieval challenge Task 2\n",
        "## Done by Ekaterina Timofeeva"
      ],
      "metadata": {
        "id": "FbphTuVRG1tz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oDuL420tGSH",
        "outputId": "5fd8825d-9f85-4094-9a2a-974229a399eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of train queries: 20\n",
            "Sample query ID: 79098180\n",
            "Gold docs for first query: ['1376881', '68722856', '7345574']\n",
            "Documents to rerank for first query: ['84117280', '86237859', '44417984', '43094034', '45901317', '42886784', '7345574', '43878837', '1354473', '86301854', '66025864', '86696791', '42365601', '86536566', '18158001', '70371603', '1376881', '81822743', '4082413', '4169775', '69328996', '68722856', '60044376', '84416764', '75998807', '42810307', '78288530', '103929967', '82465446', '87437516']\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# Utility function to load JSON\n",
        "def load_json(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "# Load data\n",
        "train_queries = load_json(\"train_queries.json\")\n",
        "gold_mappings = load_json(\"train_gold_mapping.json\")\n",
        "shuffled_ranking = load_json(\"shuffled_pre_ranking.json\")\n",
        "query_contents = load_json(\"queries_content_with_features.json\")\n",
        "document_contents = load_json(\"documents_content_with_features.json\")\n",
        "\n",
        "# Confirm data structure\n",
        "print(f\"Number of train queries: {len(train_queries)}\")\n",
        "print(f\"Sample query ID: {train_queries[0]}\")\n",
        "print(f\"Gold docs for first query: {gold_mappings[train_queries[0]]}\")\n",
        "print(f\"Documents to rerank for first query: {shuffled_ranking[train_queries[0]]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zWTUtT7tGSY",
        "outputId": "e40e5544-6945-49f1-a026-9078a6a241db"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "900"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "len(document_contents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0Y798qntGSa"
      },
      "outputs": [],
      "source": [
        "def list_to_dict_by_fan(data_list):\n",
        "    \"\"\"\n",
        "    Converts list of patent dicts into a dict keyed by patent ID (from FAN field).\n",
        "    \"\"\"\n",
        "    return {item[\"FAN\"]: item for item in data_list}\n",
        "\n",
        "# Apply conversion\n",
        "query_contents_dict = list_to_dict_by_fan(query_contents)\n",
        "document_contents_dict = list_to_dict_by_fan(document_contents)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "6uTgJoqsF7u5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ! pip install transformers torch"
      ],
      "metadata": {
        "id": "Jv4B3Cm4F-1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "from torch.nn.functional import softmax\n",
        "from tqdm import tqdm\n"
      ],
      "metadata": {
        "id": "ZcA6gXG4F-ya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer and model\n",
        "model_name = \"intfloat/e5-large-v2\"  # Baseline model\n",
        "# model_name = \"Linq-AI-Research/Linq-Embed-Mistral\"\n",
        "# model_name = \"infly/inf-retriever-v1-1.5b\"\n",
        "# model_name = \"mxbai-reranker-large-v1\"\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "model.eval()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFWavX5oF-u-",
        "outputId": "df94f721-2179-425b-c5eb-01559333dca7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at intfloat/e5-large-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 1024, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 1024)\n",
              "      (token_type_embeddings): Embedding(2, 1024)\n",
              "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-23): 24 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=1024, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tA-vesBtF-rW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "reRE9-_9F-c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Title + abstract\n",
        "\n",
        "0.211 on test"
      ],
      "metadata": {
        "id": "sZDTUx_fEqHi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNoLpHmjtGSd"
      },
      "outputs": [],
      "source": [
        "def extract_text_pairs_ta(query_ids, shuffled_ranking, query_contents, document_contents):\n",
        "    \"\"\"\n",
        "    Returns query-doc text pairs using Content['title'] + Content['pa01'] as 'TA' (Title + Abstract).\n",
        "    \"\"\"\n",
        "    data_pairs = {}\n",
        "\n",
        "    for qid in query_ids:\n",
        "        query_info = query_contents.get(qid, {}).get(\"Content\", {})\n",
        "        query_title = query_info.get(\"title\", \"\")\n",
        "        query_abstract = query_info.get(\"pa01\", \"\")\n",
        "        query_text = f\"{query_title}. {query_abstract}\"\n",
        "\n",
        "        doc_ids = shuffled_ranking[qid]\n",
        "        doc_tuples = []\n",
        "\n",
        "        for doc_id in doc_ids:\n",
        "            doc_info = document_contents.get(doc_id, {}).get(\"Content\", {})\n",
        "            doc_title = doc_info.get(\"title\", \"\")\n",
        "            doc_abstract = doc_info.get(\"pa01\", \"\")\n",
        "            doc_text = f\"{doc_title}. {doc_abstract}\"\n",
        "\n",
        "            doc_tuples.append((doc_id, query_text, doc_text))\n",
        "\n",
        "        data_pairs[qid] = doc_tuples\n",
        "\n",
        "    return data_pairs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pH_A3a5NtGSh",
        "outputId": "438140ad-b51f-4b53-c705-a107c89f649c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query Text Sample:\n",
            " Universal dispenser monitor. A retrofit dispenser monitor is disclosed. The dispenser monitor has a connector allowing it to be connected directly to one of a number of dispensers. The dispenser monitor also comprises a sensor configured to detect the dispensing action of the attached dispenser by l\n",
            "First Doc Text Sample:\n",
            " Dispenser tool, robot system with dispenser tool and method for dispensing viscous material onto wind turbine blade surface. A dispenser tool is provided with multiple cartridges for dispensing viscous material onto the surface of a wind turbine blade . The dispenser tool is advantageously part of a\n"
          ]
        }
      ],
      "source": [
        "ta_pairs = extract_text_pairs_ta(\n",
        "    query_ids=train_queries,\n",
        "    shuffled_ranking=shuffled_ranking,\n",
        "    query_contents=query_contents_dict,\n",
        "    document_contents=document_contents_dict\n",
        ")\n",
        "\n",
        "sample_qid = train_queries[0]\n",
        "print(\"Query Text Sample:\\n\", ta_pairs[sample_qid][0][1][:300])\n",
        "print(\"First Doc Text Sample:\\n\", ta_pairs[sample_qid][0][2][:300])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHTu0ApCtGSp"
      },
      "outputs": [],
      "source": [
        "def score_query_doc_pairs(model, tokenizer, data_pairs, max_length=512, batch_size=8):\n",
        "    reranked_results = {}\n",
        "\n",
        "    for query_id, doc_pairs in tqdm(data_pairs.items(), desc=\"Scoring Queries\"):\n",
        "        scores = []\n",
        "        texts = [(query, doc) for (_, query, doc) in doc_pairs]\n",
        "        doc_ids = [doc_id for (doc_id, _, _) in doc_pairs]\n",
        "\n",
        "        # Batch processing\n",
        "        for i in range(0, len(texts), batch_size):\n",
        "            batch = texts[i:i+batch_size]\n",
        "            inputs = tokenizer(\n",
        "                [f\"{q} [SEP] {d}\" for q, d in batch],\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=max_length,\n",
        "                return_tensors=\"pt\"\n",
        "            ).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "                logits = outputs.logits.squeeze(-1)\n",
        "                batch_scores = logits.cpu().numpy().tolist()\n",
        "                scores.extend(batch_scores)\n",
        "\n",
        "        # Sort doc_ids by score (descending)\n",
        "        doc_score_pairs = sorted(zip(doc_ids, scores), key=lambda x: x[1], reverse=True)\n",
        "        reranked_results[query_id] = [doc_id for doc_id, _ in doc_score_pairs]\n",
        "\n",
        "    return reranked_results\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_ta = score_query_doc_pairs(model, tokenizer, ta_pairs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwaPv-7T3hRY",
        "outputId": "a7374995-7551-4aa9-a911-bf0d44bcffd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scoring Queries: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:49<00:00,  2.46s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_map_and_recall(predictions, gold_mapping, k_values=[3, 5, 10, 20]):\n",
        "    results = {\n",
        "        \"MAP\": 0.0,\n",
        "        \"Recall@k\": {k: 0.0 for k in k_values},\n",
        "        \"Mean Rank\": 0.0,\n",
        "        \"Mean Inverse Rank\": 0.0\n",
        "    }\n",
        "\n",
        "    total_queries = 0\n",
        "    map_sum = 0.0\n",
        "    mean_rank_sum = 0.0\n",
        "    mean_inv_rank_sum = 0.0\n",
        "    recall_hits = {k: 0 for k in k_values}\n",
        "\n",
        "    for qid, ranked_docs in predictions.items():\n",
        "        if qid not in gold_mapping:\n",
        "            continue\n",
        "\n",
        "        gold_docs = set(gold_mapping[qid])\n",
        "        if not gold_docs:\n",
        "            continue\n",
        "\n",
        "        total_queries += 1\n",
        "        ap_sum = 0.0\n",
        "        hit_count = 0\n",
        "        first_hit_rank = None\n",
        "\n",
        "        for rank, doc_id in enumerate(ranked_docs, 1):\n",
        "            if doc_id in gold_docs:\n",
        "                hit_count += 1\n",
        "                ap_sum += hit_count / rank\n",
        "                if first_hit_rank is None:\n",
        "                    first_hit_rank = rank\n",
        "\n",
        "        # average precision\n",
        "        map_sum += ap_sum / len(gold_docs)\n",
        "\n",
        "        if first_hit_rank:\n",
        "            mean_rank_sum += first_hit_rank\n",
        "            mean_inv_rank_sum += 1 / first_hit_rank\n",
        "\n",
        "        # Recall@k logic\n",
        "        for k in k_values:\n",
        "            top_k = set(ranked_docs[:k])\n",
        "            if gold_docs & top_k:\n",
        "                recall_hits[k] += 1\n",
        "\n",
        "    if total_queries == 0:\n",
        "        return results\n",
        "\n",
        "    results[\"MAP\"] = map_sum / total_queries\n",
        "    results[\"Mean Rank\"] = mean_rank_sum / total_queries\n",
        "    results[\"Mean Inverse Rank\"] = mean_inv_rank_sum / total_queries\n",
        "\n",
        "    for k in k_values:\n",
        "        results[\"Recall@k\"][k] = recall_hits[k] / total_queries\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "ixjCSNjdwqTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_results = compute_map_and_recall(\n",
        "    predictions=predictions_ta,\n",
        "    gold_mapping=gold_mappings,  # from train_gold_mapping.json\n",
        "    k_values=[3, 5, 10, 20]\n",
        ")\n",
        "\n",
        "# Print metrics\n",
        "print(\"\\nðŸ” Evaluation Results on Training Data:\")\n",
        "for metric, value in eval_results.items():\n",
        "    if isinstance(value, dict):\n",
        "        for k, v in value.items():\n",
        "            print(f\"{metric}@{k}: {v:.4f}\")\n",
        "    else:\n",
        "        print(f\"{metric}: {value:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KB9YXIXz3Z-_",
        "outputId": "db4eda9a-8c5d-47ec-db98-5ecad434f338"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ” Evaluation Results on Training Data:\n",
            "MAP: 0.1876\n",
            "Recall@k@3: 0.3000\n",
            "Recall@k@5: 0.3500\n",
            "Recall@k@10: 0.7000\n",
            "Recall@k@20: 0.9500\n",
            "Mean Rank: 8.1000\n",
            "Mean Inverse Rank: 0.2380\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TA + claim 1\n",
        "\n",
        "0.242 on test"
      ],
      "metadata": {
        "id": "38wbUyskFGZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_pairs_tac1(query_ids, shuffled_ranking, query_contents, document_contents):\n",
        "    \"\"\"\n",
        "    Returns query-doc text pairs using:\n",
        "    Title + Abstract + First Claim (tac1)\n",
        "    \"\"\"\n",
        "    def get_first_claim(content_dict):\n",
        "        # Claims are labeled like 'c-en-0001', 'c-en-0002', etc.\n",
        "        for key in sorted(content_dict.keys()):\n",
        "            if key.startswith(\"c-en-\"):\n",
        "                return content_dict[key]\n",
        "        return \"\"\n",
        "\n",
        "    data_pairs = {}\n",
        "\n",
        "    for qid in query_ids:\n",
        "        query_info = query_contents.get(qid, {}).get(\"Content\", {})\n",
        "        query_title = query_info.get(\"title\", \"\")\n",
        "        query_abstract = query_info.get(\"pa01\", \"\")\n",
        "        query_first_claim = get_first_claim(query_info)\n",
        "        query_text = f\"{query_title}. {query_abstract}. {query_first_claim}\"\n",
        "\n",
        "        doc_ids = shuffled_ranking[qid]\n",
        "        doc_tuples = []\n",
        "\n",
        "        for doc_id in doc_ids:\n",
        "            doc_info = document_contents.get(doc_id, {}).get(\"Content\", {})\n",
        "            doc_title = doc_info.get(\"title\", \"\")\n",
        "            doc_abstract = doc_info.get(\"pa01\", \"\")\n",
        "            doc_first_claim = get_first_claim(doc_info)\n",
        "            doc_text = f\"{doc_title}. {doc_abstract}. {doc_first_claim}\"\n",
        "\n",
        "            doc_tuples.append((doc_id, query_text, doc_text))\n",
        "\n",
        "        data_pairs[qid] = doc_tuples\n",
        "\n",
        "    return data_pairs\n"
      ],
      "metadata": {
        "id": "q-YH4Dm_-qRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tac1_pairs = extract_text_pairs_tac1(\n",
        "    query_ids=train_queries,\n",
        "    shuffled_ranking=shuffled_ranking,\n",
        "    query_contents=query_contents_dict,\n",
        "    document_contents=document_contents_dict\n",
        ")\n"
      ],
      "metadata": {
        "id": "eeHIv6YE-w9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_tac1 = score_query_doc_pairs(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    data_pairs=tac1_pairs,\n",
        "    batch_size=4\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "eval_tac1 = compute_map_and_recall(predictions_tac1, gold_mappings)\n",
        "\n",
        "# Display metrics\n",
        "for metric, value in eval_tac1.items():\n",
        "    if isinstance(value, dict):\n",
        "        for k, v in value.items():\n",
        "            print(f\"{metric}@{k}: {v:.4f}\")\n",
        "    else:\n",
        "        print(f\"{metric}: {value:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CID-Knes-6rL",
        "outputId": "e9c54267-28dc-4d4b-8359-51baa4057f18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scoring Queries: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [01:03<00:00,  3.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAP: 0.2065\n",
            "Recall@k@3: 0.3000\n",
            "Recall@k@5: 0.6000\n",
            "Recall@k@10: 0.8000\n",
            "Recall@k@20: 0.9500\n",
            "Mean Rank: 6.9000\n",
            "Mean Inverse Rank: 0.2965\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TA + all claims\n",
        "\n",
        "0.203 on test"
      ],
      "metadata": {
        "id": "iT9kwNaWFOym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_pairs_all_claims(query_ids, shuffled_ranking, query_contents, document_contents):\n",
        "    def get_all_claims(content_dict):\n",
        "        return \" \".join(\n",
        "            content_dict[k] for k in sorted(content_dict) if k.startswith(\"c-en-\")\n",
        "        )\n",
        "\n",
        "    data_pairs = {}\n",
        "\n",
        "    for qid in query_ids:\n",
        "        query_info = query_contents.get(qid, {}).get(\"Content\", {})\n",
        "        query_title = query_info.get(\"title\", \"\")\n",
        "        query_abstract = query_info.get(\"pa01\", \"\")\n",
        "        query_claims = get_all_claims(query_info)\n",
        "        query_text = f\"{query_title}. {query_abstract}. {query_claims}\"\n",
        "\n",
        "        doc_ids = shuffled_ranking[qid]\n",
        "        doc_tuples = []\n",
        "\n",
        "        for doc_id in doc_ids:\n",
        "            doc_info = document_contents.get(doc_id, {}).get(\"Content\", {})\n",
        "            doc_title = doc_info.get(\"title\", \"\")\n",
        "            doc_abstract = doc_info.get(\"pa01\", \"\")\n",
        "            doc_claims = get_all_claims(doc_info)\n",
        "            doc_text = f\"{doc_title}. {doc_abstract}. {doc_claims}\"\n",
        "\n",
        "            doc_tuples.append((doc_id, query_text, doc_text))\n",
        "\n",
        "        data_pairs[qid] = doc_tuples\n",
        "\n",
        "    return data_pairs\n"
      ],
      "metadata": {
        "id": "JmanGqiVCbjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "claims_pairs = extract_text_pairs_all_claims(\n",
        "    query_ids=train_queries,\n",
        "    shuffled_ranking=shuffled_ranking,\n",
        "    query_contents=query_contents_dict,\n",
        "    document_contents=document_contents_dict\n",
        ")\n"
      ],
      "metadata": {
        "id": "u2mJPBhrCoTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_claims = score_query_doc_pairs(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    data_pairs=claims_pairs,\n",
        "    batch_size=4  # reduce if needed for long inputs\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YIdFia1C287",
        "outputId": "5a26e34f-7c41-499b-b6f8-5900d2871081"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scoring Queries: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [01:05<00:00,  3.29s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_claims = compute_map_and_recall(predictions_claims, gold_mappings)\n",
        "\n",
        "for metric, value in eval_claims.items():\n",
        "    if isinstance(value, dict):\n",
        "        for k, v in value.items():\n",
        "            print(f\"{metric}@{k}: {v:.4f}\")\n",
        "    else:\n",
        "        print(f\"{metric}: {value:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8o5muqlSC81z",
        "outputId": "f82956d7-0826-4620-e66c-71795f80791f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAP: 0.2208\n",
            "Recall@k@3: 0.4000\n",
            "Recall@k@5: 0.6000\n",
            "Recall@k@10: 0.9000\n",
            "Recall@k@20: 0.9500\n",
            "Mean Rank: 6.3500\n",
            "Mean Inverse Rank: 0.2618\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TAC1 + features\n",
        "\n",
        "Not submitted to codabench, since poor results on train set"
      ],
      "metadata": {
        "id": "X_pU1NLnGsv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_pairs_tac1f(query_ids, shuffled_ranking, query_contents, document_contents):\n",
        "    def get_first_claim(content_dict):\n",
        "        for key in sorted(content_dict.keys()):\n",
        "            if key.startswith(\"c-en-\"):\n",
        "                return content_dict[key]\n",
        "        return \"\"\n",
        "\n",
        "    data_pairs = {}\n",
        "\n",
        "    for qid in query_ids:\n",
        "        query_data = query_contents.get(qid, {})\n",
        "        content = query_data.get(\"Content\", {})\n",
        "        features = str(query_data.get(\"features\", \"\"))\n",
        "        query_text = f\"{content.get('title', '')}. {content.get('pa01', '')}. {get_first_claim(content)}. {features}\"\n",
        "\n",
        "        doc_ids = shuffled_ranking[qid]\n",
        "        doc_tuples = []\n",
        "\n",
        "        for doc_id in doc_ids:\n",
        "            doc_data = document_contents.get(doc_id, {})\n",
        "            content = doc_data.get(\"Content\", {})\n",
        "            features = str(doc_data.get(\"features\", \"\"))\n",
        "            doc_text = f\"{content.get('title', '')}. {content.get('pa01', '')}. {get_first_claim(content)}. {features}\"\n",
        "\n",
        "            doc_tuples.append((doc_id, query_text, doc_text))\n",
        "\n",
        "        data_pairs[qid] = doc_tuples\n",
        "\n",
        "    return data_pairs\n"
      ],
      "metadata": {
        "id": "oe_rhuVJGvwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tac1f_pairs = extract_text_pairs_tac1f(\n",
        "    query_ids=train_queries,\n",
        "    shuffled_ranking=shuffled_ranking,\n",
        "    query_contents=query_contents_dict,\n",
        "    document_contents=document_contents_dict\n",
        ")\n"
      ],
      "metadata": {
        "id": "SinibNQLHMMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_tac1f = score_query_doc_pairs(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    data_pairs=tac1f_pairs,\n",
        "    batch_size=4\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTfokuVaHSQA",
        "outputId": "cbb11580-15f3-427c-d5bc-e42268159718"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scoring Queries: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [01:00<00:00,  3.03s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_tac1f = compute_map_and_recall(predictions_tac1f, gold_mappings)\n",
        "\n",
        "for metric, value in eval_tac1f.items():\n",
        "    if isinstance(value, dict):\n",
        "        for k, v in value.items():\n",
        "            print(f\"{metric}@{k}: {v:.4f}\")\n",
        "    else:\n",
        "        print(f\"{metric}: {value:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V58joiTiHVCD",
        "outputId": "ebb2cda9-c05e-4501-91e1-5670189eee86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAP: 0.1819\n",
            "Recall@k@3: 0.2500\n",
            "Recall@k@5: 0.5500\n",
            "Recall@k@10: 0.8500\n",
            "Recall@k@20: 0.9500\n",
            "Mean Rank: 7.6000\n",
            "Mean Inverse Rank: 0.2079\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Features\n",
        "\n",
        "0.202 on test"
      ],
      "metadata": {
        "id": "3Xe2fV5MJtMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_pairs_features_only(query_ids, shuffled_ranking, query_contents, document_contents):\n",
        "    data_pairs = {}\n",
        "\n",
        "    for qid in query_ids:\n",
        "        query_text = str(query_contents.get(qid, {}).get(\"features\", \"\"))\n",
        "\n",
        "        doc_ids = shuffled_ranking[qid]\n",
        "        doc_tuples = []\n",
        "\n",
        "        for doc_id in doc_ids:\n",
        "            doc_text = str(document_contents.get(doc_id, {}).get(\"features\", \"\"))\n",
        "            doc_tuples.append((doc_id, query_text, doc_text))\n",
        "\n",
        "        data_pairs[qid] = doc_tuples\n",
        "\n",
        "    return data_pairs\n"
      ],
      "metadata": {
        "id": "x6tVg1lRJvrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_pairs = extract_text_pairs_features_only(\n",
        "    query_ids=train_queries,\n",
        "    shuffled_ranking=shuffled_ranking,\n",
        "    query_contents=query_contents_dict,\n",
        "    document_contents=document_contents_dict\n",
        ")\n"
      ],
      "metadata": {
        "id": "j8Ucs47XJ0Kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_features = score_query_doc_pairs(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    data_pairs=features_pairs,\n",
        "    batch_size=4\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_XKhXuLJ2Wk",
        "outputId": "acb69e2f-9131-4f11-8f52-b70d5484f5fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scoring Queries: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:02<00:00,  7.12it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_features = compute_map_and_recall(predictions_features, gold_mappings)\n",
        "\n",
        "for metric, value in eval_features.items():\n",
        "    if isinstance(value, dict):\n",
        "        for k, v in value.items():\n",
        "            print(f\"{metric}@{k}: {v:.4f}\")\n",
        "    else:\n",
        "        print(f\"{metric}: {value:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuHUp0_bJ4tP",
        "outputId": "41ab4af8-3eea-4cb0-8b57-d65f71ff1ede"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAP: 0.2140\n",
            "Recall@k@3: 0.4000\n",
            "Recall@k@5: 0.5500\n",
            "Recall@k@10: 0.9000\n",
            "Recall@k@20: 0.9000\n",
            "Mean Rank: 7.2000\n",
            "Mean Inverse Rank: 0.2984\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GENERATE TEST PREDICTIONS"
      ],
      "metadata": {
        "id": "Rj-GSXbWFXbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load test query IDs\n",
        "test_queries = load_json(\"test_queries.json\")"
      ],
      "metadata": {
        "id": "51lsHS64Dp3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_pairs_test = extract_text_pairs_features_only(\n",
        "    query_ids=test_queries,\n",
        "    shuffled_ranking=shuffled_ranking,\n",
        "    query_contents=query_contents_dict,\n",
        "    document_contents=document_contents_dict\n",
        ")\n",
        "\n",
        "predictions_test_features = score_query_doc_pairs(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    data_pairs=features_pairs_test,\n",
        "    batch_size=4\n",
        ")\n",
        "\n",
        "with open(\"prediction2.json\", \"w\") as f:\n",
        "    json.dump(predictions_test_features, f, indent=2)\n",
        "\n",
        "print(\"âœ… Submission file ready for test upload (features only)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "loHLQH5ZKVYi",
        "outputId": "4298c663-affa-451b-acfd-dadfe8190c9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scoring Queries: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:02<00:00,  4.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Submission file ready for test upload (features only)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# claims_pairs_test = extract_text_pairs_all_claims(\n",
        "#     query_ids=test_queries,\n",
        "#     shuffled_ranking=shuffled_ranking,\n",
        "#     query_contents=query_contents_dict,\n",
        "#     document_contents=document_contents_dict\n",
        "# )\n",
        "\n",
        "# predictions_test_claims = score_query_doc_pairs(\n",
        "#     model=model,\n",
        "#     tokenizer=tokenizer,\n",
        "#     data_pairs=claims_pairs_test,\n",
        "#     batch_size=4\n",
        "# )\n",
        "\n",
        "# with open(\"prediction2.json\", \"w\") as f:\n",
        "#     json.dump(predictions_test_claims, f, indent=2)\n",
        "\n",
        "# print(\"âœ… Test prediction saved. Ready to upload.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "il-lj5oKDiq0",
        "outputId": "5a97ce17-c581-4c1d-b108-6cf21d26d0d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scoring Queries: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:32<00:00,  3.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Test prediction saved. Ready to upload.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # ta_pairs_test = extract_text_pairs_ta(\n",
        "# #     query_ids=test_queries,\n",
        "# #     shuffled_ranking=shuffled_ranking,\n",
        "# #     query_contents=query_contents_dict,\n",
        "# #     document_contents=document_contents_dict\n",
        "# # )\n",
        "# tac1_pairs_test = extract_text_pairs_tac1(\n",
        "#     query_ids=test_queries,\n",
        "#     shuffled_ranking=shuffled_ranking,\n",
        "#     query_contents=query_contents_dict,\n",
        "#     document_contents=document_contents_dict\n",
        "# )\n",
        "# # 3. Run scoring on test pairs using your model\n",
        "# predictions_test = score_query_doc_pairs(\n",
        "#     model=model,\n",
        "#     tokenizer=tokenizer,\n",
        "#     data_pairs=tac1_pairs_test,\n",
        "#     max_length=512,\n",
        "#     batch_size=8\n",
        "# )\n",
        "\n",
        "# # 4. Save predictions to JSON\n",
        "# with open(\"prediction2.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "#     json.dump(predictions_test, f, indent=2)\n",
        "\n",
        "# print(\"âœ… Saved test predictions to prediction2.json\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDCdFRISvcYJ",
        "outputId": "30528ee5-be16-443c-ec04-ddb4e267f305"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scoring Queries: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:28<00:00,  2.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Saved test predictions to prediction2.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}